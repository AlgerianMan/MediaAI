{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BigGAN exercise solutions\n",
    "This notebook provides model solutions to the exercises of the [BigGAN test](BigGAN test.ipynb) notebook. \n",
    "\n",
    "Let's first get the network ready for sampling (this is the same code as in the exercise notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#the pylab inline may be needed to make figures visible in Jupyter, depending on your Anaconda installation\n",
    "%pylab inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" # comment this out if your tensorflow works ok with GPU\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "# Define the BigGAN model. The \"128\" means a 128x128 pixel model which is reasonably fast\n",
    "# to run without a beefy GPU. There are also 256x256 and 512x512 models available\n",
    "# Uncomment the one you want to test.\n",
    "print(\"Loading BigGAN module\")\n",
    "module = hub.Module('https://tfhub.dev/deepmind/biggan-deep-128/1')\n",
    "print(\"Module input info:\")\n",
    "print(module.get_input_info_dict())\n",
    "print(\"Module output info:\")\n",
    "print(module.get_output_info_dict())\n",
    "\n",
    "print(\"Creating the sampling ops\")\n",
    "# The generator needs two inputs: random noise (z) and ImageNet class label (y).\n",
    "# We define placeholders for these, which will make it easy to do experiments after \n",
    "# everything has been initialized\n",
    "yIn=tf.placeholder(dtype=tf.float32,shape=[None,1000])  #there are 1000 classes\n",
    "zIn=tf.placeholder(dtype=tf.float32,shape=[None,128]) #these deep BigGAN models have as many latent dimensions as pixel width and height\n",
    "truncationIn=tf.placeholder(dtype=tf.float32)#this is just a sincle floating point number\n",
    "\n",
    "# This tensor will hold the samples\n",
    "samples = module(dict(y=yIn, z=zIn, truncation=truncationIn))\n",
    "\n",
    "# As always, we need to have a Tensorflow session and initialize the variables.\n",
    "print(\"Creating the session\")\n",
    "sess=tf.Session()\n",
    "print(\"Initializing variables\")\n",
    "tf.global_variables_initializer().run(session=sess)\n",
    "print(\"Done! You should now be able to generate samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: create images that blend or crossbreed two classes\n",
    "\n",
    "This is easy: we just set the probability of multiple classes to some nonzero value. First try setting both to 1, but if one class seems to dominate, you can slightly decrease its probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# We want this many images at a time\n",
    "batch_size = 4\n",
    "\n",
    "#these deep BigGAN models have as many latent dimensions as pixel width and height\n",
    "N=128 \n",
    "\n",
    "# Define how much variety we get.\n",
    "truncation = 0.5    \n",
    "\n",
    "#The model is conditioned by the 1000-value discrete probability distribution of classes.\n",
    "#First, we initialize the distribution to zeroes\n",
    "y=np.zeros([batch_size,1000])\n",
    "# 220 and 229 are both breeds of dog, let's crossbreed\n",
    "y[:,220]=1 \n",
    "y[:,229]=1 \n",
    "\n",
    "# The latent space vector, randomized\n",
    "z = truncation*np.random.normal(size=[batch_size, N])  # random vector\n",
    "\n",
    "#Run the network, feeding the values computed above to the placeholder input tensors\n",
    "samples_fetched=sess.run(samples,feed_dict={zIn:z,yIn:y,truncationIn:truncation})\n",
    "\n",
    "#Plot\n",
    "pp.figure(1,figsize=[batch_size*4,4])\n",
    "for i in range(batch_size):\n",
    "    pp.subplot(1,batch_size,1+i)\n",
    "    pp.imshow(samples_fetched[i]*0.5+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 2: Create small variations\n",
    "\n",
    "This can be done by keeping the z of some image and then adding some noise to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Let's suppose we want variations of the first image of the batch above\n",
    "template=z[0,:]\n",
    "\n",
    "#Now, we create new z by using adding some noise. You can experiment with the noise multiplier.\n",
    "variations=template+0.2*np.random.normal(size=[batch_size, N])\n",
    "\n",
    "#Run the network, feeding the values computed above to the placeholder input tensors\n",
    "samples_fetched=sess.run(samples,feed_dict={zIn:variations,yIn:y,truncationIn:truncation})\n",
    "\n",
    "#Plot\n",
    "pp.figure(1,figsize=[batch_size*4,4])\n",
    "for i in range(batch_size):\n",
    "    pp.subplot(1,batch_size,1+i)\n",
    "    pp.imshow(samples_fetched[i]*0.5+0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Interpolate between two images\n",
    "\n",
    "Here, we need to interpolate between two pairs of z and y. The easiest way to do this is linear interpolation, i.e., \"cross-fading\" between the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#this many interpolation steps\n",
    "batch_size=10\n",
    "\n",
    "#Let's just randomly pick the z vectors\n",
    "z1=truncation*np.random.normal(size=N)  \n",
    "z2=truncation*np.random.normal(size=N)\n",
    "\n",
    "#We will use one-hot y vectors\n",
    "y1=np.zeros(1000)\n",
    "y1[229]=1\n",
    "y2=np.zeros(1000)\n",
    "y1[220]=1\n",
    "\n",
    "#Reserve space for the interpolated\n",
    "z=np.zeros([batch_size,128])\n",
    "y=np.zeros([batch_size,1000]) \n",
    "\n",
    "#Interpolate: The np.linspace() below returns a vector of batch_size elements, values sweeping from 0 to 1\n",
    "#You could also compute the mixing factor for z1,y1 inside the for loop as (i+1)/batchSize\n",
    "t=np.linspace(0,1,batch_size)\n",
    "for i in range(batch_size):\n",
    "    #we mix together z1 and z2 weighted by t[i] and (1-t[i]), i.e., the mixing weights sum to 1\n",
    "    z[i]=t[i]*z1+(1.0-t[i])*z2\n",
    "    y[i]=t[i]*y1+(1.0-t[i])*y2\n",
    "  \n",
    "#Run the network, feeding the values computed above to the placeholder input tensors\n",
    "samples_fetched=sess.run(samples,feed_dict={zIn:z,yIn:y,truncationIn:truncation})\n",
    "\n",
    "#Plot\n",
    "pp.figure(1,figsize=[batch_size*4,4])\n",
    "for i in range(batch_size):\n",
    "    pp.subplot(1,batch_size,1+i)\n",
    "    pp.imshow(samples_fetched[i]*0.5+0.5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rendering the interpolation as video\n",
    "This extension was kindly contributed by Tuure Saloheimo, spring 2019. Note that it only works if you have ffmpeg installed and you set the path correctly (the ```pp.rcParams[]``` below). On Linux, you should be able to find the path using ```which ffmpeg``` from the terminal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import components required for animation\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "\n",
    "#IMPORTANT: you must set this path correctly, depending on where you have installed ffmpeg\n",
    "#If you don't have ffmpeg, you can download it from https://ffmpeg.zeranoe.com/builds/ \n",
    "pp.rcParams['animation.ffmpeg_path'] = '/opt/conda/bin/ffmpeg'\n",
    "\n",
    "# Define starting class\n",
    "# Complete class list: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a\n",
    "y_1 = np.zeros([1000])\n",
    "y_1[24] = 1 # owl\n",
    "# Define final class\n",
    "y_2 = np.zeros([1000])\n",
    "y_2[965] = 1 # burrito\n",
    "\n",
    "# Define the number of samples to collect\n",
    "# NOTE: setting batch_size too high can result in the program running out of memory,\n",
    "#       a value of 10 or lower is recommended\n",
    "batch_count = 15\n",
    "batch_size = 4\n",
    "total_samples = batch_count * batch_size\n",
    "\n",
    "# Randomize the start and end points in the latent space\n",
    "truncation = 0.7\n",
    "z_std = 0.7\n",
    "z_1 = truncation*np.array(np.random.normal(0, z_std, size=[1,N]))\n",
    "z_2 = truncation*np.array(np.random.normal(0, z_std, size=[1,N]))\n",
    "\n",
    "# Preparations for storing images\n",
    "fig = pp.figure()\n",
    "pp.axis('off')\n",
    "ims = []\n",
    "\n",
    "# Collect all samples\n",
    "for b in range(batch_count):\n",
    "    print(\"Collecting samples, batch \"+str(b+1)+\"/\"+str(batch_count), end='\\r')\n",
    "    # Interpolate y and z values for the current batch\n",
    "    int_y = np.empty([batch_size, 1000])\n",
    "    int_z = np.empty([batch_size, 128])\n",
    "    for i in range(batch_size):\n",
    "        x2 = (i+b*batch_size)/(total_samples-1)\n",
    "        x1 = 1 - x2\n",
    "        int_y[i] = x1*y_1 + x2*y_2\n",
    "        int_z[i] = x1*z_1 + x2*z_2\n",
    "        \n",
    "    # Fetch samples\n",
    "    samples_fetched = sess.run(samples, feed_dict={zIn:int_z, yIn:int_y, truncationIn:truncation})\n",
    "    # Store the resulting images\n",
    "    for i in range(batch_size):\n",
    "        im = pp.imshow(samples_fetched[i]*0.5+0.5)\n",
    "        ims.append([im])\n",
    "\n",
    "print(\"\\nCreating animation\")\n",
    "# Define animation length in seconds\n",
    "anim_length = 5\n",
    "# Create animation\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=anim_length*1000/total_samples, blit=True)\n",
    "# Close the current plot to prevent last frame from showing as its own plot\n",
    "pp.close()\n",
    "# Display the animation\n",
    "HTML(ani.to_html5_video())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
