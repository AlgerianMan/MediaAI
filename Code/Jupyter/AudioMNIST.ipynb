{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio classification, with visualization and sonification of the results\n",
    "\n",
    "This notebook is designed as the audio-equivalent of the [MNIST image classification exercise](MNIST.ipynb). Instead of MNIST data, we use a subset of [this drum sounds dataset](http://deepyeti.ucsd.edu/cdonahue/wavegan/data/drums.tar.gz) provided as part of the course repository. \n",
    "\n",
    "As you will see, audio can be processed very similarly to images, but with two modifications:\n",
    "\n",
    "* Tensors are indexed as ```[index in batch,sample,channel]``` instead of ```[index in batch,x,y,channel]```\n",
    "* Conv1D instead of Conv2D\n",
    "\n",
    "**After you've read, run, and understood the code, try to modify it as follows to test your learning:**\n",
    "* Hard: based on the [adversarial training example](AdversarialMNIST.ipynb), try optimizing a sound that maximizes the activation output of some neuron. This might yield interesting sound textures.\n",
    "\n",
    "Currently, we have no easy modifications to suggest. Feel free to invent your own!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and playing the data\n",
    "\n",
    "First, let's load the dataset using a helper provided for the course and try playing a loaded sound in IPython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#The pylab inline below is something you may need to make images and plots visible in Jupyter, depending on your Anaconda setup\n",
    "%pylab inline  \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, a simple example like this runs faster on CPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from helpers.audio_loader import load_audio \n",
    "import IPython \n",
    "print(os.getcwd())\n",
    "\n",
    "#Load the audio dataset. The course repository includes a small subset of this drum sound set: \n",
    "#http://deepyeti.ucsd.edu/cdonahue/wavegan/data/drums.tar.gz\n",
    "nClasses=3\n",
    "(x_train, y_train), (x_test, y_test) = load_audio(\"drums_subset_preprocessed\", num_classes=nClasses, inputpath=\"../Datasets/\")\n",
    "\n",
    "#Play a sound. \n",
    "#IPython.display.Audio can play audio stored as Numpy arrays if you specify the sampling rate.\n",
    "#The method expects 1D vectors, which is why we need to index as [sample number,:,channel],\n",
    "#where the : denotes that we play the whole sound and not just a part of it.\n",
    "IPython.display.Audio(x_train[0,:,0],rate=16000,autoplay=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Let's import the layer types we need\n",
    "from tensorflow.keras.layers import Dense   #fully connected layer\n",
    "from tensorflow.keras.layers import Flatten #converts images to vectors of numbers\n",
    "\n",
    "#As before, we use a simply sequential, i.e., multilayer architecture\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#Flatten converts a batch of multidimensional data into a batch of 1D data. \n",
    "#This is what the fully connected layers expect.\n",
    "#For example, the rows of an image are simply stacked after each other.\n",
    "#If the data was not images, we would not need this.\n",
    "model.add(Flatten())\n",
    "\n",
    "#The audio classification is so much harder that we need to have at least a\n",
    "#few layers. Just the final 3-neuron layer, we won't learn anything\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "model.add(Dense(64,activation=\"relu\"))\n",
    "\n",
    "#The output layer is fully connected, with 1 neuron for each 10 classes.\n",
    "#For classification, one should use the softmax activation.\n",
    "#This means that each output neuron can be thought as the probability of a class.\n",
    "model.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "#Compile the model. Note that now y_test is one-hot vectors instead of indices.\n",
    "#Thus, categorical_crossentropy loss instead of sparse_categorical_crossentropy.\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train the network\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=10,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Sonify some of the first layer neuron weights\n",
    "#First, query the weights. We use index 1 because index 0 is the flatten layer\n",
    "weights=model.layers[1].get_weights()[0]\n",
    "#Create a figure with appropriate size\n",
    "nNeuronsToSonify=10\n",
    "#Loop over the neurons\n",
    "for i in range(nNeuronsToSonify):\n",
    "    #Weights is a 2D tensor where the first dimension indexes over data variables, second over neurons\n",
    "    sound=weights[:,i]\n",
    "    #Reshape and play\n",
    "    sound=np.reshape(sound,sound.shape[0])\n",
    "    #Here, as we want to show multiple audio playback widgets, we have to use\n",
    "    #the IPython.display.display() around the IPython.display.Audio()\n",
    "    IPython.display.display(IPython.display.Audio(sound,rate=16000))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A convolutional neural network for audio\n",
    "The classification accuracy of the fully connected network is very poor. Let's try the same with a convolutional neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Let's import the layer types we need\n",
    "#import tensorflow.keras.layers.Layer\n",
    "from tensorflow.keras.layers import Dense   #fully connected layer\n",
    "from tensorflow.keras.layers import Conv1D  #convolutional layer with 1D filters\n",
    "from tensorflow.keras.layers import Flatten #converts images to plain vectors of numbers\n",
    "from tensorflow.keras.layers import Dropout #this mitigates overfitting\n",
    "\n",
    "#As before, we use a simply sequential, i.e., multilayer architecture\n",
    "model = keras.models.Sequential()\n",
    "\n",
    "#First convolutional layer. Here, the kernels are 1D tensors, and we can use\n",
    "#9 wide ones instead of the 5x5 and 3x3 we used in image classification. \n",
    "#Larger kernels are typically a bit better but use much more computing resources.\n",
    "#With 1D convolutions, the cost \n",
    "kernelSize=9\n",
    "model.add(Conv1D(16, kernel_size=kernelSize, strides=2,activation='relu',\n",
    "                 input_shape=(x_train.shape[1],x_train.shape[2],)))\n",
    "\n",
    "#Now, let's add more convolutional layers until the temporal dimension of the output is small enough\n",
    "while model.layers[-1].output.shape[1]>kernelSize*2:\n",
    "    model.add(Conv1D(32, kernel_size=kernelSize, activation='relu', strides=2))\n",
    "    \n",
    "#Fully connected part\n",
    "model.add(Flatten())\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(nClasses, activation='softmax'))\n",
    "\n",
    "#Compile the model. \n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.Adam(),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "#Train the network\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=32,\n",
    "          epochs=5,\n",
    "          verbose=1,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the classifier with a sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#this is the test image\n",
    "testIdx=82\n",
    "\n",
    "#We index by testIdx:testIdx+1 to pass a batch of one image to the network instead of just one image\n",
    "classProbabilities=model.predict(x_test[testIdx:testIdx+1])\n",
    "print(\"Predicted class probabilities: \",classProbabilities)\n",
    "\n",
    "#np.argmax returns the index of the largest value in a Numpy tensor.\n",
    "#np.max returns the largest value\n",
    "classes=[\"Kick\",\"Ride\",\"Snare\"]\n",
    "print(\"The most probable class is {}, with probability {}\".format(classes[np.argmax(classProbabilities)],np.max(classProbabilities)))\n",
    "\n",
    "IPython.display.Audio(x_test[testIdx,:,0],rate=16000,autoplay=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to see what the network has learned. With image classification, we could show the convolution filters as images. Here, we can't simply play them as audio, as they are only 5 values, whereas a second of audio is 16000 values in our dataset. Thus, we simply plot them as curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#Define a visualization helper\n",
    "\n",
    "def visualizeLayerWeights(model,layerIndex):\n",
    "    #Get the neuron weights, i.e., convolution kernels or filters\n",
    "    kernel=model.layers[layerIndex].get_weights()[0]\n",
    "    #Check the shape\n",
    "    print(\"Visualizing layer {} kernel, shape {}\".format(layerIndex,kernel.shape))\n",
    "    #Visualize 16 first filters\n",
    "    nFiltersToVisualize=16\n",
    "    pp.figure(1,figsize=[nFiltersToVisualize*2,2])  #specify figsize explicitly because otherwise the images will be too small\n",
    "    for i in range(nFiltersToVisualize):\n",
    "        pp.subplot(1,nFiltersToVisualize,1+i)\n",
    "        pp.plot(kernel[:,0,i])\n",
    "    pp.show()\n",
    "    \n",
    "#visualize first layer\n",
    "visualizeLayerWeights(model,0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
