{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the solution example for the exercises given in [PredictWeight.ipynb](PredictWeight.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First exercise: Data normalization using StandardScaler\n",
    "\n",
    "Data loading works similarly as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, a simple example like this runs faster on CPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "import pandas as pd\n",
    "\n",
    "import pandas as pd\n",
    "dataframe=pd.read_csv(\"https://raw.githubusercontent.com/PerttuHamalainen/MediaAI/master/Code/Datasets/weight-height.csv\")\n",
    "data=np.array(dataframe)\n",
    "data=data[:,1:]\n",
    "data=data.astype(np.float) \n",
    "data[:,0]*=2.54   \n",
    "data[:,1]*=0.45359237\n",
    "pp.scatter(data[:,0],data[:,1],marker=\".\")\n",
    "pp.title(\"Relation of height and weight\")\n",
    "pp.xlabel(\"Height (centimeters)\")\n",
    "pp.ylabel(\"Weight (kilograms)\")\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to optimize the data for neural networks, we make it zero-mean, unit standard deviation using the StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler=StandardScaler()\n",
    "scaler.fit(data)\n",
    "scaled=scaler.transform(data)\n",
    "pp.scatter(scaled[:,0],scaled[:,1],marker=\".\")\n",
    "pp.title(\"Normalized data\")\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data, training the neural network will be much faster. We will first train a single-neuron network, i.e., a simple linear model. You should see that the model is pretty good already after 5 epochs, even though we use a smaller learning rate of 0.01, which allows the model to fit more accurately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#keras.Sequential makes it easy to compose a neural network models out of layers\n",
    "model = keras.Sequential()\n",
    "\n",
    "#Add a 1-neuron layer with linear activation, taking one input value. \n",
    "#The input_shape=(1,) defines that there's only a single input value, but batch size is yet unknown.\n",
    "#Note that this notation is a bit misleading, as the batch data index dimension is really the first one and not the second one.  \n",
    "#Fortunately, the input_shape needs to only be specified for the first layer\n",
    "model.add(keras.layers.Dense(1,input_shape=(1,)))\n",
    "\n",
    "#Make the model ready for optimization using Adam optimizer (the usual reasonable first guess).\n",
    "#The loss parameter defines the loss function that optimization tries to minimize, in this case\n",
    "#the mean squared error between the network outputs and actual data values.\n",
    "#The lr parameter is the \"learning rate\". With this simple model, we can use a high learning rate of 0.1,\n",
    "#whereas many complex networks require 0.001 or even 0.0001. This makes training more stable but also more slow.\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.01),loss=\"mean_squared_error\")\n",
    "\n",
    "#Define our training inputs and outputs. Our network takes in height (column 0 in the data) and outputs weight (column 1).\n",
    "trainingInputs=scaled[:,0]\n",
    "trainingOutputs=scaled[:,1]\n",
    "\n",
    "#Reshape the tensors: This is needed because Tensorflow and Keras models expect to get data in batches, as specified above.\n",
    "trainingInputs=np.reshape(trainingInputs,[trainingInputs.shape[0],1])\n",
    "trainingOutputs=np.reshape(trainingOutputs,[trainingOutputs.shape[0],1])\n",
    "\n",
    "#Fit (train) the model. Epochs defines how many times the network will see all data during the training.\n",
    "model.fit(trainingInputs,trainingOutputs,verbose=1,epochs=5)\n",
    "\n",
    "#Plot the data and predictions given by the network\n",
    "pp.scatter(trainingInputs[:,0],trainingOutputs[:,0],marker=\".\")\n",
    "pp.title(\"Relation of height and weight\")\n",
    "pp.xlabel(\"Height (centimeters)\")\n",
    "pp.ylabel(\"Weight (kilograms)\")\n",
    "predictions=model.predict(trainingInputs)\n",
    "#NOTE: The predictions is of the same shape as trainingOutputs, i.e., [10000,1]\n",
    "#scatter() expects 1-dimensional x and y arrays; thus, we need to use the [:,0] and [:,1] indexing.\n",
    "pp.scatter(trainingInputs[:,0],predictions[:,0])\n",
    "pp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that when using this model for predicting real, non-scaled values, we must use the scaler. There's a few ways to go about it. If you want to stick to the transform() and inverse_transform() functions, it can be a bit cumbersome, as one needs to always think of what kind of tensor shapes are fed in to networks and functions and what shapes are received as output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#We want to predict weight from this height:\n",
    "height=200\n",
    "#To get normalized height, we must compose a [1,2] shaped tensor for the scaler, i.e., \n",
    "#1 pair of heights and weights. Height is the first value and the second value can be anything (we use 0).\n",
    "normalizedHeight=scaler.transform([[height,0]])\n",
    "#The scaler returns a tensor of similar shape, let's get the actual height value out of it\n",
    "normalizedHeight=normalizedHeight[0,0]\n",
    "#Now, to get a normalized weight, we can feed this to the network.\n",
    "#Again, the returned value is a batch, so we take the first element\n",
    "normalizedWeight=model.predict([[normalizedHeight]])[0,0]\n",
    "#Finally, we can use the scaler to get unnormalized \n",
    "weight=scaler.inverse_transform([[normalizedHeight,normalizedWeight]])\n",
    "#The scaler returns a tensor of similar shape, let's get the actual weight value out of it\n",
    "weight=weight[0,1]\n",
    "print(\"The predicted weight for a person whose height is\",height,\"cm is\",weight,\"kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you feel more comfortable doing the calculations yourself, you can directly access the scaler's mean and variance. The standard deviation equals the square root of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "normalizedHeight=(200-scaler.mean_[0])/np.sqrt(scaler.var_[0])\n",
    "normalizedWeight=model.predict([[normalizedHeight]])[0,0]\n",
    "weight=normalizedWeight*np.sqrt(scaler.var_[1])+scaler.mean_[1]\n",
    "print(\"The predicted weight for a person whose height is 200 cm is\",weight,\"kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second exercise: use a more complex neural network. \n",
    "\n",
    "To see things in more detail and get to demonstrate overfitting, we will only use the first 50 data points. Overfitting can be mitigated by using a larger dataset, a more simple model, or a regularization technique such as dropout.\n",
    "\n",
    "Here, overfitting should manifest as a nonlinear model, even though the data is from a (noisy) linear relation.\n",
    "\n",
    "To gain a better feel of overfitting, you can try different data, layer, and neuron counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential()\n",
    "#add a layer with 32 ReLU neurons\n",
    "model.add(keras.layers.Dense(4,activation=\"relu\",input_shape=(1,)))\n",
    "#add a layer with 32 ReLU neurons\n",
    "model.add(keras.layers.Dense(4,activation=\"relu\"))\n",
    "#NOTE: we don't need to specify input_shape for others than the first layer. Keras can deduce it automatically.\n",
    "#add the output layer (1 neuron because only 1 predicted value)\n",
    "model.add(keras.layers.Dense(1))\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.01),loss=\"mean_squared_error\")\n",
    "\n",
    "#Define our training inputs and outputs. Our network takes in height (column 0 in the data) and outputs weight (column 1).\n",
    "trainingInputs=scaled[:50,0]\n",
    "trainingOutputs=scaled[:50,1]\n",
    "\n",
    "#Reshape the tensors: This is needed because Tensorflow and Keras models expect to get data in batches, as specified above.\n",
    "trainingInputs=np.reshape(trainingInputs,[trainingInputs.shape[0],1])\n",
    "trainingOutputs=np.reshape(trainingOutputs,[trainingOutputs.shape[0],1])\n",
    "\n",
    "#Fit the model. Epochs defines how many times the network will see all data during the training.\n",
    "model.fit(trainingInputs,trainingOutputs,verbose=2,epochs=100)\n",
    "\n",
    "#Scatterplot both the data and the predictions\n",
    "pp.scatter(trainingInputs,trainingOutputs)\n",
    "predictions=model.predict(trainingInputs)\n",
    "pp.scatter(trainingInputs,predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
