{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "char_rnn.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4T2cBVI0s1g",
        "colab_type": "text"
      },
      "source": [
        "# A Char-RNN Implementation in Tensorflow\n",
        "*This notebook is slightly modified from https://colab.research.google.com/drive/13Vr3PrDg7cc4OZ3W2-grLSVSf0RJYWzb, with the following changes:*\n",
        "\n",
        "* Main parameters defined at the start instead of middle\n",
        "* Run all works, because of the added upload_custom_data parameter\n",
        "* Training time specified in minutes instead of steps, for time-constrained classroom use\n",
        "\n",
        "---\n",
        "CharRNN was a well known generative text model (character level LSTM) created by Andrej Karpathy. It allowed easy training and generation of arbitrary text with many hilarious results:\n",
        "\n",
        "  * Music: abc notation\n",
        "<https://highnoongmt.wordpress.com/2015/05/22/lisls-stis-recurrent-neural-networks-for-folk-music-generation/>,\n",
        "  * Irish folk music\n",
        "<https://soundcloud.com/seaandsailor/sets/char-rnn-composes-irish-folk-music>-\n",
        "  * Obama speeches\n",
        "<https://medium.com/@samim/obama-rnn-machine-generated-political-speeches-c8abd18a2ea0>-\n",
        "  * Eminem lyrics\n",
        "<https://soundcloud.com/mrchrisjohnson/recurrent-neural-shady>- (NSFW ;-))\n",
        "  * Research awards\n",
        "<http://karpathy.github.io/2015/05/21/rnn-effectiveness/#comment-2073825449>-\n",
        "  * TED Talks\n",
        "<https://medium.com/@samim/ted-rnn-machine-generated-ted-talks-3dd682b894c0>-\n",
        "  * Movie Titles <http://www.cs.toronto.edu/~graves/handwriting.html>\n",
        "  \n",
        "This notebook contains a reimplementation in Tensorflow. It will let you input a file containing the text you want your generator to mimic, train your model, see the results, and save it for future use.\n",
        "\n",
        "To get started, start running the cells in order, following the instructions at each step. You will need a sizable text file (try at least 1 MB of text) when prompted to upload one. For exploration you can also use the provided text corpus taken from Shakespeare's works.\n",
        "\n",
        "The training cell saves a checkpoint every 30 seconds, so you can check the output of your network and not lose any progress.\n",
        "\n",
        "## Outline\n",
        "\n",
        "This notebook will guide you through the following steps. Roughly speaking, these will be our steps: \n",
        "  * Upload some data\n",
        "  * Set some training parameters (you can just use the defaults for now)\n",
        "  * Define our Model, training loss function, and data input manager\n",
        "  * Train on a cloud GPU\n",
        "  * Save out model and use it to generate some new text.\n",
        "  \n",
        "Design of the RNN is inspired by [this github project](https://github.com/sherjilozair/char-rnn-tensorflow) which was based on Andrej Karpathy's [char-rnn](https://github.com/karpathy/char-rnn). If you'd like to learn more, Andrej's [blog post](http://karpathy.github.io/2015/05/21/rnn-effectiveness/) is a great place to start."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhBCvDKnVstE",
        "colab_type": "text"
      },
      "source": [
        "### Imports and Values Needed to Run this Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "67qDGHYCzj6v",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "from __future__ import absolute_import, print_function, division\n",
        "from google.colab import files\n",
        "from collections import Counter, defaultdict\n",
        "from copy import deepcopy\n",
        "from IPython.display import clear_output\n",
        "from random import randint\n",
        "\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "CHECKPOINT_DIR = './checkpoints/'  #Checkpoints are temporarily kept here.\n",
        "TEXT_ENCODING = 'utf-8'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LVEdNYclTSdv",
        "colab_type": "text"
      },
      "source": [
        "### Let's define our training parameters.\n",
        "Feel free to leave these untouched at their default values and just run this cell as is. Later, you can come back here and experiment wth these. \n",
        "These parameters are just for training. Further down at the inference step, we'll define parameters for the text-generation step."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRPTh7_A2u80",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#The most common parameters to change\n",
        "upload_custom_data = False  #if false, use the default Shakespeare data\n",
        "training_time_minutes = 2 #change this depending on how much time you have\n",
        "\n",
        "#Neural network and optimization default parameters that usually work ok\n",
        "num_layers = 2\n",
        "state_size = 256\n",
        "batch_size = 64\n",
        "sequence_length = 256\n",
        "steps_per_epoch = 500\n",
        "learning_rate = 0.002\n",
        "learning_rate_decay = 0.95\n",
        "gradient_clipping = 5.0\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBtu02V2ocXC",
        "colab_type": "text"
      },
      "source": [
        "### Get the training data.\n",
        "\n",
        "We can either download the works of Shakespeare to train on or upload our own plain text file that we will be training on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FvoepQadkPG6",
        "colab_type": "code",
        "outputId": "97e6fb99-8416-4c2e-f434-e4f9e6a7b4aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 317
        }
      },
      "source": [
        "if not upload_custom_data:\n",
        "  shakespeare_url = \"https://ocw.mit.edu/ans7870/6/6.006/s08/lecturenotes/files/t8.shakespeare.txt\"\n",
        "  import urllib\n",
        "  file_contents = urllib.urlopen(shakespeare_url).read()\n",
        "  file_name = \"shakespeare\"\n",
        "  file_contents = file_contents[10501:]  # Skip headers and start at content\n",
        "  print(\"An excerpt: \\n\", file_contents[:664])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "An excerpt: \n",
            "                      1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7anKDCqMkrQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if upload_custom_data:\n",
        "  uploaded = files.upload()\n",
        "  if type(uploaded) is not dict: uploaded = uploaded.files  ## Deal with filedit versions\n",
        "  file_bytes = uploaded[uploaded.keys()[0]]\n",
        "  utf8_string = file_bytes.decode(TEXT_ENCODING)\n",
        "  file_contents = utf8_string if files else ''\n",
        "  file_name = uploaded.keys()[0]\n",
        "  print(\"An excerpt: \\n\", file_contents[:664])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO0NDmM0VgQU",
        "colab_type": "text"
      },
      "source": [
        "## Set up the recurrent LSTM network \n",
        "\n",
        "Before we can do anything, we have to define what our neural network looks like. This next cell creates a class which will contain the tensorflow graph and training parameters that make up the network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAK1D26NKGpJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RNN(object):\n",
        "  \"\"\"Represents a Recurrent Neural Network using LSTM cells.\n",
        "\n",
        "  Attributes:\n",
        "    num_layers: The integer number of hidden layers in the RNN.\n",
        "    state_size: The size of the state in each LSTM cell.\n",
        "    num_classes: Number of output classes. (E.g. 256 for Extended ASCII).\n",
        "    batch_size: The number of training sequences to process per step.\n",
        "    sequence_length: The number of chars in a training sequence.\n",
        "    batch_index: Index within the dataset to start the next batch at.\n",
        "    on_gpu_sequences: Generates the training inputs for a single batch.\n",
        "    on_gpu_targets: Generates the training labels for a single batch.\n",
        "    input_symbol: Placeholder for a single label for use during inference.\n",
        "    temperature: Used when sampling outputs. A higher temperature will yield\n",
        "      more variance; a lower one will produce the most likely outputs. Value\n",
        "      should be between 0 and 1.\n",
        "    initial_state: The LSTM State Tuple to initialize the network with. This\n",
        "      will need to be set to the new_state computed by the network each cycle.\n",
        "    logits: Unnormalized probability distribution for the next predicted\n",
        "      label, for each timestep in each sequence.\n",
        "    output_labels: A [batch_size, 1] int32 tensor containing a predicted\n",
        "      label for each sequence in a batch. Only generated in infer mode.\n",
        "  \"\"\"\n",
        "  def __init__(self,\n",
        "               rnn_num_layers=1,\n",
        "               rnn_state_size=128,\n",
        "               num_classes=256,\n",
        "               rnn_batch_size=1,\n",
        "               rnn_sequence_length=1):\n",
        "    self.num_layers = rnn_num_layers\n",
        "    self.state_size = rnn_state_size\n",
        "    self.num_classes = num_classes\n",
        "    self.batch_size = rnn_batch_size\n",
        "    self.sequence_length = rnn_sequence_length\n",
        "    self.batch_shape = (self.batch_size, self.sequence_length)\n",
        "    print(\"Built LSTM: \",\n",
        "          self.num_layers ,self.state_size ,self.num_classes ,\n",
        "          self.batch_size ,self.sequence_length ,self.batch_shape)\n",
        "\n",
        "\n",
        "  def build_training_model(self, dropout_rate, data_to_load):\n",
        "    \"\"\"Sets up an RNN model for running a training job.\n",
        "\n",
        "    Args:\n",
        "      dropout_rate: The rate at which weights may be forgotten during training.\n",
        "      data_to_load: A numpy array of containing the training data, with each\n",
        "        element in data_to_load being an integer representing a label. For\n",
        "        example, for Extended ASCII, values may be 0 through 255.\n",
        "\n",
        "    Raises:\n",
        "      ValueError: If mode is data_to_load is None.\n",
        "    \"\"\"\n",
        "    if data_to_load is None:\n",
        "      raise ValueError('To continue, you must upload training data.')\n",
        "    inputs = self._set_up_training_inputs(data_to_load)\n",
        "    self._build_rnn(inputs, dropout_rate)\n",
        "\n",
        "  def build_inference_model(self):\n",
        "    \"\"\"Sets up an RNN model for generating a sequence element by element.\n",
        "    \"\"\"\n",
        "    self.input_symbol = tf.placeholder(shape=[1, 1], dtype=tf.int32)\n",
        "    self.temperature = tf.placeholder(shape=(), dtype=tf.float32,\n",
        "                                      name='temperature')\n",
        "    self.num_options = tf.placeholder(shape=(), dtype=tf.int32,\n",
        "                                      name='num_options')\n",
        "    self._build_rnn(self.input_symbol, 0.0)\n",
        "\n",
        "    self.temperature_modified_logits = tf.squeeze(\n",
        "        self.logits, 0) / self.temperature\n",
        "\n",
        "    #for beam search\n",
        "    self.normalized_probs = tf.nn.softmax(self.logits)\n",
        "\n",
        "    self.output_labels = tf.multinomial(self.temperature_modified_logits,\n",
        "                                        self.num_options)\n",
        "\n",
        "  def _set_up_training_inputs(self, data):\n",
        "    self.batch_index = tf.placeholder(shape=(), dtype=tf.int32)\n",
        "    batch_input_length = self.batch_size * self.sequence_length\n",
        "\n",
        "    input_window = tf.slice(tf.constant(data, dtype=tf.int32),\n",
        "                            [self.batch_index],\n",
        "                            [batch_input_length + 1])\n",
        "\n",
        "    self.on_gpu_sequences = tf.reshape(\n",
        "        tf.slice(input_window, [0], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    self.on_gpu_targets = tf.reshape(\n",
        "        tf.slice(input_window, [1], [batch_input_length]), self.batch_shape)\n",
        "\n",
        "    return self.on_gpu_sequences\n",
        "\n",
        "  def _build_rnn(self, inputs, dropout_rate):\n",
        "    \"\"\"Generates an RNN model using the passed functions.\n",
        "\n",
        "    Args:\n",
        "      inputs: int32 Tensor with shape [batch_size, sequence_length] containing\n",
        "        input labels.\n",
        "      dropout_rate: A floating point value determining the chance that a weight\n",
        "        is forgotten during evaluation.\n",
        "    \"\"\"\n",
        "    # Alias some commonly used functions\n",
        "    dropout_wrapper = tf.contrib.rnn.DropoutWrapper\n",
        "    lstm_cell = tf.contrib.rnn.LSTMCell\n",
        "    multi_rnn_cell = tf.contrib.rnn.MultiRNNCell\n",
        "\n",
        "    self._cell = multi_rnn_cell(\n",
        "        [dropout_wrapper(lstm_cell(self.state_size), 1.0, 1.0 - dropout_rate)\n",
        "         for _ in range(self.num_layers)])\n",
        "\n",
        "    self.initial_state = self._cell.zero_state(self.batch_size, tf.float32)\n",
        "\n",
        "    embedding = tf.get_variable('embedding',\n",
        "                                [self.num_classes, self.state_size])\n",
        "\n",
        "    embedding_input = tf.nn.embedding_lookup(embedding, inputs)\n",
        "    output, self.new_state = tf.nn.dynamic_rnn(self._cell, embedding_input,\n",
        "                                               initial_state=self.initial_state)\n",
        "\n",
        "    self.logits = tf.contrib.layers.fully_connected(output, self.num_classes,\n",
        "                                                    activation_fn=None)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJ2NdzM5RHyK",
        "colab_type": "text"
      },
      "source": [
        "###Define your loss function\n",
        "Loss is a measure of how well the neural network is modeling the data distribution. \n",
        "\n",
        "Pass in your logits and the targets you're training against. In this case, target_weights is a set of multipliers that will put higher emphasis on certain outputs. In this notebook, we'll give all outputs equal importance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gbBZuBT6NhVG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_loss(logits, targets, target_weights):\n",
        "  with tf.name_scope('loss'):\n",
        "    return tf.contrib.seq2seq.sequence_loss(\n",
        "        logits,\n",
        "        targets,\n",
        "        target_weights,\n",
        "        average_across_timesteps=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Svir_HYvpHQn",
        "colab_type": "text"
      },
      "source": [
        "### Define your optimizer\n",
        "This tells Tensorflow how to reduce the loss. We will use the popular [ADAM algorithm](https://www.tensorflow.org/api_docs/python/tf/train/AdamOptimizer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swLcZqsePGAG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_optimizer(loss, initial_learning_rate, gradient_clipping, global_step,\n",
        "                  decay_steps, decay_rate):\n",
        "\n",
        "  with tf.name_scope('optimizer'):\n",
        "    computed_learning_rate = tf.train.exponential_decay(\n",
        "        initial_learning_rate,\n",
        "        global_step,\n",
        "        decay_steps,\n",
        "        decay_rate,\n",
        "        staircase=True)\n",
        "\n",
        "    optimizer = tf.train.AdamOptimizer(computed_learning_rate)\n",
        "    trained_vars = tf.trainable_variables()\n",
        "    gradients, _ = tf.clip_by_global_norm(\n",
        "        tf.gradients(loss, trained_vars),\n",
        "        gradient_clipping)\n",
        "    training_op = optimizer.apply_gradients(\n",
        "        zip(gradients, trained_vars),\n",
        "        global_step=global_step)\n",
        "\n",
        "    return training_op, computed_learning_rate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60O5Dw_Hr5-s",
        "colab_type": "text"
      },
      "source": [
        "### This class will let us view the progress of our training as it progresses."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmfVg_eEeaOq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class LossPlotter(object):\n",
        "  def __init__(self, history_length):\n",
        "    self.global_steps = []\n",
        "    self.losses = []\n",
        "    self.averaged_loss_x = []\n",
        "    self.averaged_loss_y = []\n",
        "    self.history_length = history_length\n",
        "\n",
        "  def draw_plots(self):\n",
        "    self._update_averages(self.global_steps, self.losses,\n",
        "                          self.averaged_loss_x, self.averaged_loss_y)\n",
        "\n",
        "    plt.title('Average Loss Over Time')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(self.averaged_loss_x, self.averaged_loss_y, label='Loss/Time (Avg)')\n",
        "    plt.plot()\n",
        "    plt.plot(self.global_steps, self.losses,\n",
        "             label='Loss/Time (Last %d)' % self.history_length,\n",
        "             alpha=.1, color='r')\n",
        "    plt.plot()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    plt.title('Loss for the last 100 Steps')\n",
        "    plt.xlabel('Global Step')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.plot(self.global_steps, self.losses,\n",
        "             label='Loss/Time (Last %d)' % self.history_length, color='r')\n",
        "    plt.plot()\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    # The notebook will be slowed down at the end of training if we plot the\n",
        "    # entire history of raw data. Plot only the last 100 steps of raw data,\n",
        "    # and the average of each 100 batches. Don't keep unused data.\n",
        "    self.global_steps = []\n",
        "    self.losses = []\n",
        "    self.learning_rates = []\n",
        "\n",
        "  def log_step(self, global_step, loss):\n",
        "    self.global_steps.append(global_step)\n",
        "    self.losses.append(loss)\n",
        "\n",
        "  def _update_averages(self, x_list, y_list,\n",
        "                       averaged_data_x, averaged_data_y):\n",
        "    averaged_data_x.append(x_list[-1])\n",
        "    averaged_data_y.append(sum(y_list) / self.history_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CPwFDEiZhr6x",
        "colab_type": "text"
      },
      "source": [
        "## Now, we're going to start training our model.\n",
        "\n",
        "This could take a while, so you might want to grab a coffee. Every 30 seconds of training, we're going to save a checkpoint to make sure we don't lose our progress. To monitor the progress of your training, feel free to stop the training every once in a while and run the inference cell to generate text with your model!\n",
        "\n",
        "First, we will need to turn the plain text file into arrays of tokens (and, later,  back). To do this we will use this token mapper helper class:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n98dKVTzkmpi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "class TokenMapper(object):\n",
        "  def __init__(self):\n",
        "    self.token_mapping = {}\n",
        "    self.reverse_token_mapping = {}\n",
        "  def buildFromData(self, utf8_string, limit=0.00004):\n",
        "    print(\"Build token dictionary.\")\n",
        "    total_num = len(utf8_string)\n",
        "    sorted_tokens = sorted(Counter(utf8_string.decode('utf8')).items(), \n",
        "                           key=lambda x: -x[1])\n",
        "    # Filter tokens: Only allow printable characters (not control chars) and\n",
        "    # limit to ones that are resonably common, i.e. skip strange esoteric \n",
        "    # characters in order to reduce the dictionary size.\n",
        "    filtered_tokens = filter(lambda t: t[0] in string.printable or \n",
        "                             float(t[1])/total_num > limit, sorted_tokens)\n",
        "    tokens, counts = zip(*filtered_tokens)\n",
        "    self.token_mapping = dict(zip(tokens, range(len(tokens))))\n",
        "    for c in string.printable:\n",
        "      if c not in self.token_mapping:\n",
        "        print(\"Skipped token for: \", c)\n",
        "    self.reverse_token_mapping = {\n",
        "        val: key for key, val in self.token_mapping.items()}\n",
        "    print(\"Created dictionary: %d tokens\"%len(self.token_mapping))\n",
        "  \n",
        "  def mapchar(self, char):\n",
        "    if char in self.token_mapping:\n",
        "      return self.token_mapping[char]\n",
        "    else:\n",
        "      return self.token_mapping[' ']\n",
        "  \n",
        "  def mapstring(self, utf8_string):\n",
        "    return [self.mapchar(c) for c in utf8_string]\n",
        "  \n",
        "  def maptoken(self, token):\n",
        "    return self.reverse_token_mapping[token]\n",
        "  \n",
        "  def maptokens(self, int_array):\n",
        "    return ''.join([self.reverse_token_mapping[c] for c in int_array])\n",
        "  \n",
        "  def size(self):\n",
        "    return len(self.token_mapping)\n",
        "  \n",
        "  def alphabet(self):\n",
        "    return ''.join([k for k,v in sorted(self.token_mapping.items(),key=itemgetter(1))])\n",
        "\n",
        "  def print(self):\n",
        "    for k,v in sorted(self.token_mapping.items(),key=itemgetter(1)): print(k, v)\n",
        "  \n",
        "  def save(self, path):\n",
        "    with open(path, 'wb') as json_file:\n",
        "      json.dump(self.token_mapping, json_file)\n",
        "  \n",
        "  def restore(self, path):\n",
        "    with open(path, 'r') as json_file:\n",
        "      self.token_mapping = {}\n",
        "      self.token_mapping.update(json.load(json_file))\n",
        "      self.reverse_token_mapping = {val: key for key, val in self.token_mapping.items()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8XTGD7fxfEuK",
        "colab_type": "text"
      },
      "source": [
        "Now convert the raw input into a list of tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RiHe0bUo9eP",
        "colab_type": "code",
        "outputId": "569c5f65-7477-455e-fafe-690b9cac3c78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "# Clean the checkpoint directory and make a fresh one\n",
        "!rm -rf {CHECKPOINT_DIR}\n",
        "!mkdir {CHECKPOINT_DIR}\n",
        "!ls -lt\n",
        "\n",
        "chars_in_batch = (sequence_length * batch_size)\n",
        "file_len = len(file_contents)\n",
        "unique_sequential_batches = file_len // chars_in_batch\n",
        "\n",
        "mapper = TokenMapper()\n",
        "mapper.buildFromData(file_contents)\n",
        "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
        "\n",
        "input_values = mapper.mapstring(file_contents)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 8\n",
            "drwxr-xr-x 2 root root 4096 Apr 17 06:03 checkpoints\n",
            "drwxr-xr-x 1 root root 4096 Apr  3 16:24 sample_data\n",
            "Build token dictionary.\n",
            "Skipped token for:  #\n",
            "Skipped token for:  $\n",
            "Skipped token for:  %\n",
            "Skipped token for:  *\n",
            "Skipped token for:  +\n",
            "Skipped token for:  /\n",
            "Skipped token for:  =\n",
            "Skipped token for:  @\n",
            "Skipped token for:  \\\n",
            "Skipped token for:  ^\n",
            "Skipped token for:  {\n",
            "Skipped token for:  ~\n",
            "Skipped token for:  \t\n",
            "Skipped token for:  \n",
            "Skipped token for:  \u000b\n",
            "Skipped token for:  \f\n",
            "Created dictionary: 84 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5He_ECNJc61",
        "colab_type": "text"
      },
      "source": [
        "###First, we'll build our neural network and add our training operations to the Tensorflow graph. \n",
        "If you're continuing training after testing your generator, run the next three cells."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgXvABhpJa1f",
        "colab_type": "code",
        "outputId": "7c80f91e-5884-42e7-d000-cc5a7bc75386",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 656
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "print('Constructing model...')\n",
        "\n",
        "model = RNN(\n",
        "    rnn_num_layers=num_layers,\n",
        "    rnn_state_size=state_size,\n",
        "    num_classes=mapper.size(),\n",
        "    rnn_batch_size=batch_size,\n",
        "    rnn_sequence_length=sequence_length)\n",
        "\n",
        "model.build_training_model(0.05, np.asarray(input_values))\n",
        "print('Constructed model successfully.')\n",
        "\n",
        "print('Setting up training session...')\n",
        "neutral_target_weights = tf.constant(\n",
        "    np.ones(model.batch_shape),\n",
        "    tf.float32\n",
        ")\n",
        "loss = get_loss(model.logits, model.on_gpu_targets, neutral_target_weights)\n",
        "global_step = tf.get_variable('global_step', shape=(), trainable=False,\n",
        "                              dtype=tf.int32)\n",
        "training_step, computed_learning_rate = get_optimizer(\n",
        "    loss,\n",
        "    learning_rate,\n",
        "    gradient_clipping,\n",
        "    global_step,\n",
        "    steps_per_epoch,\n",
        "    learning_rate_decay\n",
        ")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0417 06:03:09.974482 140218974406528 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0417 06:03:09.977370 140218974406528 deprecation.py:323] From <ipython-input-12-12171759f484>:109: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0417 06:03:09.984956 140218974406528 deprecation.py:323] From <ipython-input-12-12171759f484>:109: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0417 06:03:10.008152 140218974406528 deprecation.py:323] From <ipython-input-12-12171759f484>:118: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0417 06:03:10.049892 140218974406528 deprecation.py:323] From /tensorflow-1.15.2/python2.7/tensorflow_core/python/ops/rnn_cell_impl.py:958: add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0417 06:03:10.059854 140218974406528 deprecation.py:506] From /tensorflow-1.15.2/python2.7/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Constructing model...\n",
            "Built LSTM:  2 256 84 64 256 (64, 256)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0417 06:03:10.153100 140218974406528 deprecation.py:323] From /tensorflow-1.15.2/python2.7/tensorflow_core/contrib/layers/python/layers/layers.py:1866: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Constructed model successfully.\n",
            "Setting up training session...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0417 06:03:10.410675 140218974406528 deprecation.py:323] From /tensorflow-1.15.2/python2.7/tensorflow_core/python/ops/clip_ops.py:301: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XNKzEmaRj5SF",
        "colab_type": "text"
      },
      "source": [
        "The supervisor will manage the training flow and checkpointing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t90StbXJj4jt",
        "colab_type": "code",
        "outputId": "1b26a381-176e-4da8-e422-e56d5458d9a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 112
        }
      },
      "source": [
        "# Create a supervisor that will checkpoint the model in the CHECKPOINT_DIR\n",
        "sv = tf.train.Supervisor(\n",
        "    logdir=CHECKPOINT_DIR,\n",
        "    global_step=global_step,\n",
        "    save_model_secs=30)\n",
        "print('Training session ready.')"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0417 06:03:10.512609 140218974406528 deprecation.py:323] From <ipython-input-19-4ba04cba093f>:4: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training session ready.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tfXYyumK5z6",
        "colab_type": "text"
      },
      "source": [
        "###This next cell will begin the training cycle. \n",
        "First, we will attempt to pick up training where we left off, if a previous checkpoint exists, then continue the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1wfsDMuLLUr3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "637f8012-c5e7-42c2-f3d7-e0c132225595"
      },
      "source": [
        "from datetime import datetime\n",
        "start_time = datetime.now()\n",
        "\n",
        "with sv.managed_session(config=config) as sess:\n",
        "  print('Training supervisor successfully initialized all variables.')\n",
        "  if not file_len:\n",
        "    raise ValueError('To continue, you must upload training data.')\n",
        "  elif file_len < chars_in_batch:\n",
        "    raise ValueError('To continue, you must upload a larger set of data.')\n",
        "\n",
        "  plotter = LossPlotter(100)\n",
        "  step_number = sess.run(global_step)\n",
        "  zero_state = sess.run([model.initial_state])\n",
        "  max_batch_index = (unique_sequential_batches - 1) * chars_in_batch\n",
        "  while not sv.should_stop() and (datetime.now()-start_time).seconds/60 < training_time_minutes:\n",
        "    feed_dict = {\n",
        "        model.batch_index: randint(0, max_batch_index),\n",
        "        model.initial_state: zero_state\n",
        "        }\n",
        "    [_, _, training_loss, step_number, current_learning_rate, _] = sess.run(\n",
        "        [model.on_gpu_sequences,\n",
        "         model.on_gpu_targets,\n",
        "         loss,\n",
        "         global_step,\n",
        "         computed_learning_rate,\n",
        "         training_step],\n",
        "        feed_dict)\n",
        "    plotter.log_step(step_number, training_loss)\n",
        "    if step_number % 100 == 0:\n",
        "      clear_output(True)\n",
        "      plotter.draw_plots()\n",
        "      print('Latest checkpoint is: %s' %\n",
        "            tf.train.latest_checkpoint(CHECKPOINT_DIR))\n",
        "      print('Learning Rate is: %f' %\n",
        "            current_learning_rate)\n",
        "\n",
        "    if step_number % 10 == 0:\n",
        "      print('global step %d, loss=%f' % (step_number, training_loss))\n",
        "\n",
        "clear_output(True)\n",
        "\n",
        "print('Training completed in HH:MM:SS = ', datetime.now()-start_time)\n",
        "print('Latest checkpoint is: %s' %\n",
        "      tf.train.latest_checkpoint(CHECKPOINT_DIR))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training completed in HH:MM:SS =  0:01:00.333687\n",
            "Latest checkpoint is: ./checkpoints/model.ckpt-100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rpr5zK1UUlEd",
        "colab_type": "text"
      },
      "source": [
        "## Now, we're going to generate some text!\n",
        "\n",
        "Here, we'll use the **Beam Search** algorithm to generate some text with our trained model. Beam Search picks N possible next options from each of the current options at every step. This way, if the generator picks an item leading to a bad decision down the line, it can toss the bad result out and keep going with a more likely one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc8W1I0bJy-6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BeamSearchCandidate(object):\n",
        "  \"\"\"Represents a node within the search space during Beam Search.\n",
        "\n",
        "  Attributes:\n",
        "    state: The resulting RNN state after the given sequence has been generated.\n",
        "    sequence: The sequence of selections leading to this node.\n",
        "    probability: The probability of the sequence occurring, computed as the sum\n",
        "      of the probabilty of each character in the sequence at its respective\n",
        "      step.\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, init_state, sequence, probability):\n",
        "    self.state = init_state\n",
        "    self.sequence = sequence\n",
        "    self.probability = probability\n",
        "\n",
        "  def search_from(self, tf_sess, rnn_model, temperature, num_options):\n",
        "    \"\"\"Expands the num_options most likely next elements in the sequence.\n",
        "\n",
        "    Args:\n",
        "      tf_sess: The Tensorflow session containing the rnn_model.\n",
        "      rnn_model: The RNN to use to generate the next element in the sequence.\n",
        "      temperature: Modifies the probabilities of each character, placing\n",
        "        more emphasis on higher probabilities as the value approaches 0.\n",
        "      num_options: How many potential next options to expand from this one.\n",
        "\n",
        "    Returns: A list of BeamSearchCandidate objects descended from this node.\n",
        "    \"\"\"\n",
        "    expanded_set = []\n",
        "    feed = {rnn_model.input_symbol: np.array([[self.sequence[-1]]]),\n",
        "            rnn_model.initial_state: self.state,\n",
        "            rnn_model.temperature: temperature,\n",
        "            rnn_model.num_options: num_options}\n",
        "    [predictions, probabilities, new_state] = tf_sess.run(\n",
        "        [rnn_model.output_labels,\n",
        "         rnn_model.normalized_probs,\n",
        "         rnn_model.new_state], feed)\n",
        "    # Get the indices of the num_beams next picks\n",
        "    picks = [predictions[0][x] for x in range(len(predictions[0]))]\n",
        "    for new_char in picks:\n",
        "      new_seq = deepcopy(self.sequence)\n",
        "      new_seq.append(new_char)\n",
        "      expanded_set.append(\n",
        "          BeamSearchCandidate(new_state, new_seq,\n",
        "                              probabilities[0][0][new_char] + self.probability))\n",
        "    return expanded_set\n",
        "\n",
        "  def __eq__(self, other):\n",
        "    return self.sequence == other.sequence\n",
        "\n",
        "  def __ne__(self, other):\n",
        "    return not self.__eq__(other)\n",
        "\n",
        "  def __hash__(self):\n",
        "    return hash(self.sequence())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97aGkvJGNFUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search_generate_sequence(tf_sess, rnn_model, primer, temperature=0.85,\n",
        "                                  termination_condition=None, num_beams=5):\n",
        "  \"\"\"Implements a sequence generator using Beam Search.\n",
        "\n",
        "  Args:\n",
        "    tf_sess: The Tensorflow session containing the rnn_model.\n",
        "    rnn_model: The RNN to use to generate the next element in the sequence.\n",
        "    temperature: Controls how 'Creative' the generated sequence is. Values\n",
        "      close to 0 tend to generate the most likely sequence, while values\n",
        "      closer to 1 generate more original sequences. Acceptable values are\n",
        "      within (0, 1].\n",
        "    termination_condition: A function taking one parameter, a list of\n",
        "      integers, that returns True when a condition is met that signals to the\n",
        "      RNN to return what it has generated so far.\n",
        "    num_beams: The number of possible sequences to keep at each step of the\n",
        "      generation process.\n",
        "\n",
        "  Returns: A list of at most num_beams BeamSearchCandidate objects.\n",
        "  \"\"\"\n",
        "  candidates = []\n",
        "\n",
        "  rnn_current_state = sess.run([rnn_model.initial_state])\n",
        "  #Initialize the state for the primer\n",
        "  for primer_val in primer[:-1]:\n",
        "    feed = {rnn_model.input_symbol: np.array([[primer_val]]),\n",
        "            rnn_model.initial_state: rnn_current_state\n",
        "           }\n",
        "    [rnn_current_state] = tf_sess.run([rnn_model.new_state], feed)\n",
        "\n",
        "  candidates.append(BeamSearchCandidate(rnn_current_state, primer, num_beams))\n",
        "\n",
        "  while True not in [termination_condition(x.sequence) for x in candidates]:\n",
        "    new_candidates = []\n",
        "    for candidate in candidates:\n",
        "      expanded_candidates = candidate.search_from(\n",
        "          tf_sess, rnn_model, temperature, num_beams)\n",
        "      for new in expanded_candidates:\n",
        "        if new not in new_candidates:\n",
        "          #do not reevaluate duplicates\n",
        "          new_candidates.append(new)\n",
        "    candidates = sorted(new_candidates,\n",
        "                        key=lambda x: x.probability, reverse=True)[:num_beams]\n",
        "\n",
        "  return [c for c in candidates if termination_condition(c.sequence)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qj0w_Vgs-oMJ",
        "colab_type": "text"
      },
      "source": [
        "Input something to start your generated text with, and set how characters long you want the text to be.\n",
        "\"Creativity\" refers to how much emphasis your neural network puts on matching a pattern. If you notice looping in the output, try raising this value. If your output seems too random, try lowering it a bit.\n",
        "If the results don't look too great in general, run the three training cells again for a bit longer. The lower your loss, the more closely your generated text will match the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAsyTCZvdFn5",
        "colab_type": "code",
        "outputId": "27b0e997-c5dd-4f9e-97db-a26e388caa4e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 150
        }
      },
      "source": [
        "tf.reset_default_graph()\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "sess = tf.InteractiveSession(config=config)\n",
        "\n",
        "model = RNN(\n",
        "    rnn_num_layers=num_layers,\n",
        "    rnn_state_size=state_size,\n",
        "    num_classes=mapper.size(),\n",
        "    rnn_batch_size=1,\n",
        "    rnn_sequence_length=1)\n",
        "\n",
        "model.build_inference_model()\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "saver = tf.train.Saver(tf.global_variables())\n",
        "ckpt = tf.train.latest_checkpoint(CHECKPOINT_DIR)\n",
        "saver.restore(sess, ckpt)\n",
        "\n",
        "def gen(start_with, pred, creativity):\n",
        "  int_array = mapper.mapstring(start_with)\n",
        "  candidates = beam_search_generate_sequence(\n",
        "      sess, model, int_array, temperature=creativity,\n",
        "      termination_condition=pred,\n",
        "      num_beams=1)\n",
        "  gentext = mapper.maptokens(candidates[0].sequence)\n",
        "  return gentext\n",
        "\n",
        "def lengthlimit(n):\n",
        "  return lambda text: len(text)>n\n",
        "def sentences(n):\n",
        "  return lambda text: mapper.maptokens(text).count(\".\")>=n\n",
        "def paragraph():\n",
        "  return lambda text: mapper.maptokens(text).count(\"\\n\")>0\n",
        "\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Built LSTM:  2 256 84 1 1 (1, 1)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/tensorflow-1.15.2/python2.7/tensorflow_core/python/client/session.py:1750: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
            "  warnings.warn('An interactive session is already active. This can '\n",
            "W0417 06:04:11.213624 140218974406528 deprecation.py:323] From <ipython-input-12-12171759f484>:75: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09ccehpqVKR8",
        "colab_type": "code",
        "cellView": "both",
        "outputId": "8323ede2-1a69-499d-927b-2c3a1ffd0936",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 769
        }
      },
      "source": [
        "length_of_generated_text = 2000\n",
        "creativity = 0.85  # Should be greater than 0 but less than 1\n",
        "\n",
        "print(gen(\"  ANTONIO: Who is it ?\", lengthlimit(length_of_generated_text), creativity))"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  ANTONIO: Who is it ?\n",
            "  SOANSCR.\n",
            " Fondr he susdels\n",
            "  F I hollin'cpy sof hiwcice thhime, or hou Whouy she met to rerepy Toucoxt y roelt thas coot tid wonlt\n",
            "    he eweund an so the't\n",
            "                                  xa lir, he we the as  soreth , I the thod tam hor miam,\n",
            "    Ald ad ceeth in be.\n",
            "\n",
            "   LENOTSAT ly tl toed sath wey wative thrr ther mo him, tove wesy I meake lir tor calceto halled hros yhag hess oitt befgl wilk tan oot that wacr wam  me thaln peure sopt.  Co an'nh roariges tir trethe; hhe. thtary hiad avedth.\n",
            "   As he the oot ant mild,\n",
            "    thyint mend\n",
            "   Bill frallesd not mochet thoat cirh onsit pillith art Gweed,\n",
            "     he ilt the hope th, of asdand, ant ork waf to banth al as menttictale od, naaklolent an.\n",
            "    I fast hiot ar ont lothe bethe\n",
            " \n",
            "  BASSARTONLLR oP her hers oit moed te goul thy bu sathees\n",
            "             Ther we lfoll won an woplot ous polet so ssaI nane kere-eind\n",
            "   If gor thag ho thtiw pon soud the monnt meoch on fher or one the wof mard oll't she havr,\n",
            "     thalt Cor anv wand by oos Gom,\n",
            "   Pole anfeod -he hoall the doul dlloth ilel be mos mely coas',\n",
            "                                                     hith or the sasd ceth at harttiny Ay mige freld othont sant wth syerert'm,\n",
            "     he thensest; dang oyhoos He\n",
            "\n",
            "  T Ime bede id bath tome thed heles fo thy wheliska pous heunils I pout therind mirve nlen anve dewlate thes werd hin?\n",
            " AANnGOTYSA. Be nor buod ten sours ang.\n",
            "   An to tham Gos the thosise thang; we the batot vece?\n",
            "   Cand anveads\n",
            "    Hist,              Fhms mpenteD\n",
            "   Fad thot yaned,\n",
            "                            2yot whes Lerent cant thes\n",
            "   Bog and wiut tte bad bo toum are hod blnnltt the th o the amy hand,\n",
            "   The hill thor of nnast\n",
            "     thoit dy lots dit tavn geen heas wout woe nhen cor I annto lr of tan eulcortt soe so wame himl brtoe nand tels houd male lafd,\n",
            "     hi him.\n",
            "   Help thatok foure wopidny?\n",
            "    And coy thes tiln, ho wees wo ve toun he siuf! thee her mas,\n",
            "     [ Arr yere thretle f pey vieve,\n",
            "   Fad Bigh inore an wary syand\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "klTwA-RrFwR0",
        "colab_type": "text"
      },
      "source": [
        "## Let's save a copy of our trained RNN so we can do all kinds of cool things with it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hdxjJaayFuhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "save_model_to_drive = False  ## Set this to true to save directly to Google Drive.\n",
        "\n",
        "def save_model_hyperparameters(path):\n",
        "  with open(path, 'w')  as json_file:\n",
        "    model_params = {\n",
        "        'num_layers': model.num_layers,\n",
        "        'state_size': model.state_size,\n",
        "        'num_classes': model.num_classes\n",
        "    }\n",
        "    json.dump(model_params, json_file)\n",
        "\n",
        "def save_to_drive(title, content):\n",
        "  # Install the PyDrive wrapper & import libraries.\n",
        "  !pip install -U -q PyDrive\n",
        "  from pydrive.auth import GoogleAuth\n",
        "  from pydrive.drive import GoogleDrive\n",
        "  from google.colab import auth\n",
        "  from oauth2client.client import GoogleCredentials\n",
        "\n",
        "  # Authenticate and create the PyDrive client.\n",
        "  auth.authenticate_user()\n",
        "  gauth = GoogleAuth()\n",
        "  gauth.credentials = GoogleCredentials.get_application_default()\n",
        "  drive = GoogleDrive(gauth)\n",
        "\n",
        "  newfile = drive.CreateFile({'title': title})\n",
        "  newfile.SetContentFile(content)\n",
        "  newfile.Upload()\n",
        "  print('Uploaded file with ID %s as %s'% (newfile.get('id'),\n",
        "         archive_name))\n",
        "    \n",
        "archive_name = ''.join([file_name,'_seedbank_char-rnn.zip'])\n",
        "latest_model = tf.train.latest_checkpoint(CHECKPOINT_DIR).split('/')[2]\n",
        "checkpoints_archive_path = ''.join(['./exports/',archive_name])\n",
        "if not latest_model:\n",
        "  raise ValueError('You must train a model before you can export one.')\n",
        "  \n",
        "%system mkdir exports\n",
        "%rm -f {checkpoints_archive_path}\n",
        "mapper.save(''.join([CHECKPOINT_DIR, 'token_mapping.json']))\n",
        "save_model_hyperparameters(''.join([CHECKPOINT_DIR, 'model_attributes.json']))\n",
        "%system zip '{checkpoints_archive_path}' -@ '{CHECKPOINT_DIR}checkpoint' \\\n",
        "            '{CHECKPOINT_DIR}token_mapping.json' \\\n",
        "            '{CHECKPOINT_DIR}model_attributes.json' \\\n",
        "            '{CHECKPOINT_DIR}{latest_model}.'*\n",
        "\n",
        "if save_model_to_drive:\n",
        "  save_to_drive(archive_name, checkpoints_archive_path)\n",
        "else:\n",
        "  files.download(checkpoints_archive_path)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}