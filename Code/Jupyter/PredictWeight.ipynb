{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is designed as one's first machine learning experiment. We will load and visualize a dataset of human heights and weights, and use a simple model to predict weight from height.\n",
    "\n",
    "**After you've read, run, and understood the code, try to modify it as follows:**\n",
    "* Easy: predict height from weight\n",
    "* Easy: Add data normalization using a sklearn StandardScaler, as you learned in [the previous tutorial](DataAndTensors.ipynb). This should make optimization much faster because the initial neural network parameters are closer to optimal. **It's a good rule of thumb that a neural network's inputs should have approximately zero mean and unit standard deviation.**\n",
    "* Harder: Test how using a too complex network for simple data can lead to overfitting and nonsensical predictions. Use data normalization as above and only the 50 first datapoints of the dataset. Use a network with two 32-neuron ReLU layers. You can add the first layer as ```model.add(keras.layers.Dense(32,activation=\"relu\",input_shape=(1,)))``` and the next layers similarly, but without specifying the input_shape, which Keras can figure out by itself.\n",
    "\n",
    "Model solutions are provided in the same folder, but first try to solve the problem yourself, at least for a few minutes. *Think of this as a puzzle game with optional hints that you will check only if you have to.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by including the basic utilities. Now, we also need tensorflow in addition to numpy and pyplot. We also import Keras because it makes building and training basic neural networks easy.\n",
    "\n",
    "For a bit more complex but similar example, see https://www.tensorflow.org/tutorials/keras/basic_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as pp\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\" #disable Tensorflow GPU usage, a simple example like this runs faster on CPU\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras  \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the data and convert to metric system, similar to the previous tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataframe=pd.read_csv(\"https://raw.githubusercontent.com/PerttuHamalainen/MediaAI/master/Code/Datasets/weight-height.csv\")\n",
    "data=np.array(dataframe)\n",
    "data=data[:,1:]\n",
    "data=data.astype(np.float) \n",
    "data[:,0]*=2.54   \n",
    "data[:,1]*=0.45359237\n",
    "pp.scatter(data[:,0],data[:,1],marker=\".\")\n",
    "pp.title(\"Relation of height and weight\")\n",
    "pp.xlabel(\"Height (centimeters)\")\n",
    "pp.ylabel(\"Weight (kilograms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To predict weight from height, we're going to need a machine learning model. We will first train a single-neuron network, i.e., a simple linear model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#keras.Sequential makes it easy to compose a neural network models out of layers\n",
    "model = keras.Sequential()\n",
    "\n",
    "#Add a 1-neuron layer with linear activation, taking one input value. \n",
    "#The input_shape=(1,) defines that there's only a single input value, but batch size is yet unknown.\n",
    "#Note that this notation is a bit misleading, as the batch data index dimension is really the first one and not the second one.  \n",
    "#Fortunately, the input_shape needs to only be specified for the first layer\n",
    "model.add(keras.layers.Dense(1,input_shape=(1,)))\n",
    "\n",
    "#Make the model ready for optimization using Adam optimizer (the usual reasonable first guess).\n",
    "#The loss parameter defines the loss function that optimization tries to minimize, in this case\n",
    "#the mean squared error between the network outputs and actual data values.\n",
    "#The lr parameter is the \"learning rate\". With this simple model, we can use a high learning rate of 0.1,\n",
    "#whereas many complex networks require 0.001 or even 0.0001. This makes training more stable but also more slow.\n",
    "model.compile(optimizer=keras.optimizers.Adam(lr=0.1),loss=\"mean_squared_error\")\n",
    "\n",
    "#Define our training inputs and outputs. Our network takes in height (column 0 in the data) and outputs weight (column 1).\n",
    "trainingInputs=data[:,0]\n",
    "trainingOutputs=data[:,1]\n",
    "\n",
    "#Reshape the tensors: This is needed because Tensorflow and Keras models expect to get data in batches, \n",
    "#as specified above.\n",
    "#Reshaping does not change tensor contents, it just changes how the contents are indexed.\n",
    "#Tensor of shapes [10], [10,1], and [1,10] can all have the same contents, but if one considers\n",
    "#the Tensorflow convention of first dimension being the batch index, shape [10,1] denotes a batch of 10\n",
    "#1D tensors, each containing a single number. Shape [1,10], in contrast, denotes a batch of a single\n",
    "#1D tensor containing 10 numbers.\n",
    "#Note that sometimes, Tensorflow can automatically convert shapes, but more often not, which is why we practice reshaping here. \n",
    "#You can try prining out the tensors before and after the reshaping!\n",
    "trainingInputs=np.reshape(trainingInputs,[trainingInputs.shape[0],1])\n",
    "trainingOutputs=np.reshape(trainingOutputs,[trainingOutputs.shape[0],1])\n",
    "\n",
    "#Fit (train) the model. Epochs defines how many times the network will see all data during the training.\n",
    "model.fit(trainingInputs,trainingOutputs,verbose=1,epochs=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the data again, adding the model predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "pp.figure(1)\n",
    "#pp.scatter(data[:,0],data[:,1],marker=\".\")\n",
    "pp.scatter(trainingInputs[:,0],trainingOutputs[:,0],marker=\".\")\n",
    "pp.title(\"Relation of height and weight\")\n",
    "pp.xlabel(\"Height (centimeters)\")\n",
    "pp.ylabel(\"Weight (kilograms)\")\n",
    "predictions=model.predict(trainingInputs)\n",
    "#NOTE: The predictions is of the same shape as trainingOutputs, i.e., [10000,1]\n",
    "#scatter() expects 1-dimensional x and y arrays; thus, we need to use the [:,0] and [:,1] indexing.\n",
    "pp.scatter(trainingInputs[:,0],predictions[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What you should see above is a line that predicts growing weight with growing height, but it does not yet fit the data really well. However, the fit gets better when the training continues. Below, we optimize for 50 more epochs and then visualize again. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#fit the model\n",
    "model.fit(trainingInputs,trainingOutputs,verbose=2,epochs=50)  \n",
    "\n",
    "#visualize the data\n",
    "pp.figure(1)\n",
    "pp.scatter(data[:,0],data[:,1],marker=\".\")\n",
    "pp.title(\"Relation of height and weight\")\n",
    "pp.xlabel(\"Height (centimeters)\")\n",
    "pp.ylabel(\"Weight (kilograms)\")\n",
    "predictions=model.predict(trainingInputs)\n",
    "pp.scatter(trainingInputs[:,0],predictions[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, it's easy to query the model with any height values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "print(\"The predicted weight for a person whose height is 200 cm is\",model.predict([[200]])[0,0],\"kg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that above, we had to use [[200]], because we only had one value to predict but the network wants a 2D tensor as input. [[200]] defines an array of shape [1,1] with the only element having indices [0,0] and value 200. Similarly, the prediction is a [1,1] shaped tensor, which we query with indices [0,0].\n",
    "\n",
    "**Unfortunately, this tensor indexing mess is one of the biggest hurdles in getting into ML coding. Don't worry, it's hard for everyone at first. Once you've wrapped your head around it, things will get much easier.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key takeaways\n",
    "\n",
    "* In machine learning, data is typically processed in (mini)batches. Thus, even if the data is 1D, the tensors passed to a neural network are 2D. In image processing, we typically have 4D tensors with shape [nBatch,height,width,nChannels]. In audio, we have the same if the data is encoded as spectograms. For raw waveforms, one typically uses [nBatch,nSamples,nChannels].\n",
    "* Tensor math operations like division or multiplication are performed elementwise. If you want to use linear algebra operations like a dot product in Numpy, you must explicitly type np.dot(tensor_a,tensor_b).\n",
    "* Numpy code often utilizes *broadcasting*. Broadcasting expands tensors with only 1 element such that tensor math operations are possible to compute. For example, one can multiply together tensors of shape [4,4] and [4,1]; the latter is expanded to a [4,4] as if it was stacked 4 times along the second dimension. However, shapes [4,4] and [4] cannot be multiplied together, even though tensors of shapes [4] and [4,1] can contain exactly the same data! If you want to perform such multiplication, you must first explicitly reshape the [4] tensor to a [4,1] using np.reshape()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
